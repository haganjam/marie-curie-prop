---
title: "minimal-working-example"
format: html
editor: source
---

### Basic approach

This is a minimal working example for how the **AIRIES** (*Adaptive Invasion-Risk Intelligence for Protected Areas with eDNA Surveillance*) project is envisioned to work.

Consider a single protected area (e.g. "pa_1") and a single invasive species ("sp_1"). Moreover, let's consider a simple one-dimensional landscape with 10 grid-cells or "sites" where "site_1", "site_2" and "site_3" form the protected area (i.e. "pa_1"):

```{r}
# number of locations
n_loc <- 100

# number of pa_cells
n_pa <- 5

# create a 1-dimension landscape
grid <- dplyr::tibble(spatial_location = runif(n = n_loc, 0, 10))

# arrange by spatial location
grid <- dplyr::arrange(grid, spatial_location)

# add a site name
grid$site <- paste0("site_", 1:n_loc)

# add protected area identity
grid$pa <- "pa_1"

# add a variable with pa_in
grid$pa_in <- c(rep(1, n_pa), rep(0, n_loc - n_pa))

# arrange columns
grid <- dplyr::select(grid, site, spatial_location, pa, pa_in)

# check these data
head(grid)
```

This data.frame shows 10 sites ("site_1" to "site_10") at a numeric spatial location for "pa_1" and shows that "site_1", "site_2" and "site_3" are within "pa_1" (i.e. pa_in == 1). The data is set-up in this way so that it is easy to estimate the influence of all grid-cells on a given protected area.

Next, we define a simple dispersal kernel based on how far each grid-cell is from the focal protected area (i.e. "pa_1"). For this, we first calculate the distance between each site and the centroid of the focal protected area (in one-dimension, the centroid is simply the mean). Grid-cells that comprise the protected area (i.e. "site_1", "site_2" and "site_3") are given distances from the focal protected area of 0. 

```{r}
# calculate distance
grid$dist_pa <- abs(grid$spatial_location - mean(grid$spatial_location[grid$pa_in == 1]))
grid$dist_pa <- with(grid, ifelse(pa_in == 1, 0, spatial_location))

# check the data
head(grid)
```

Using this distance, we define a kernel weight $K(i, p) \epsilon [0, 1]$ which describes how dispersal from grid-cell $i$ contributs to protected area $p$ if grid-cell $i$ is within protected area $p$ (otherwise, a value of 1 is given for $K(i, p)$):

$$
K(i, p) = exp(-d(i, p)/\alpha)
$$

```{r}
# define dispersal distance alpha
alpha <- 1.5

# implement the dispersal kernel
grid$k_ip <- with(grid,
                  ifelse(dist_pa == 0, 1, exp(-(dist_pa/alpha))))

# check the data
head(grid)
```

We are evalusating risk to the PA from cells outside the PA. Therefore, we remove the PAs and, therefore, conduct the rest of the analysis for grid-cells not present in the PA:

```{r}
# keep a copy of the PA data for plotting
grid_pa <- dplyr::filter(grid, pa_in == 1)

# extract sites not present in the pa from the grid
grid <- dplyr::filter(grid, pa_in == 0)
```


The next piece of information is the prior distribution that grid-cell $i$ is occupied by the focal species. We would typically derive this using a logistic-regression model of known presence-absence data (i.e. a kind of species distribution model) fit in a Bayesian framework where $y_i$ is the presence (1) or absence (0) of the species in grid-cell i. Therefore, we would approximate the posterior probability distribution $p(\psi_i | y_i)$ using posterior samples ($\psi_i^{(s)} \text{ where }s = 1,...,S$). Note that while this is technically a posterior distribution, we will use it as the **prior distribution** of species occupancy for each grid-cell.

We draw the samples ($\psi_i^{(s)} \text{ where }s = 1,...,S$) from a Beta distribution. The mean of a beta distribution is given by a mean ($\mu$) and a precision parameter ($\kappa$):

$$
\mu = \frac{\alpha}{\alpha + \beta}
$$

$$
\kappa = \alpha + \beta
$$

The function in R `rbeta` is parameterised unsing the $\alpha$ and $\beta$ parameters. Therefore, to parameterise them in terms of the ($\mu$) and ($\kappa$), we use the following formulae:

$$
\alpha = \mu + \kappa
$$

$$
\beta = \kappa \times(1 - \mu)
$$

Therefore, we can define a formula to convert the $\mu$ and $\kappa$ values to $\alpha$ and $\beta$ values which makes it easier to parameterise the Beta() distribution.

```{r}
# define the function
rbeta_parameters <- function(parameter, mu, kappa) {
  if (parameter == "alpha") {
    mu * kappa
  } else if (parameter == "beta") {
    kappa * (1 - mu)
  } else {
    stop("choose an appropriate parameter argument value")
  }
}
```

```{r}
# set these probabilities manually and use a beta-distribution
mu <- seq(0.05, 0.25, length.out = nrow(grid))
kappa <- 25

# generate the psi-values
psi_list <-
  lapply(mu, function(x) {
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = x, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = x, kappa = kappa)) |> 
      round(5)
})

# name the list by site
names(psi_list) <- grid$site
```

For each grid-cell and each sample of species occupancy ($\psi_i^{(s)} \text{ where }s = 1,...,S$), we define each grid-cell's contribution to the risk to the focal protected area ("pa_1" in this case) as:

$$
R_{i,p}^{(s)} = \psi_i^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

$$
\text{where: } \psi_i^{(s)} \text{ where }s = 1,...,S
$$

Where:

$\psi_i^{(s)}$ is the probability that a focal species is present in cell $i$ given sample $s$ \\ 
$w_{species}$ is a weight describing the potential ecological damage of the species \\ 
$w_{site}$ is a weight describing the ecological value of the protected area \\ 
$K_{i,p}$ is the dispersal kernel

```{r}
# set the species weight
w_species <- 0.5

# set the site weight
w_site <- 0.8

# calculate the risk values for each cell
R_list <- list()
for (i in seq_len(nrow(grid))) {
  R_list[[i]] <- psi_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_list) <- grid$site

# check the output
R_list[[1]]
```

This leaves us with estimates for two probability distributions for each grid-cell $i$ based on the samples from the posterior distribution:

$$
p(\psi_i) \sim \psi_i^{(s)} \text{ where }s = 1,...,S
$$

$$
p(R_{i,p}) \sim  R_{i,p}^{(s)} 
$$

Because these are valid probability distributions that represent uncertainty, we can subject them to information-theoretical concepts. For example, we can calculate the entropy associated with the risk ($R_{i,p}$) or occupancy ($\psi_i$) of a given grid-cell $i$ in protected area $p$. For example:

$$
H(R_{i,p}) = -\int p(R_{i,p}) \cdot log(p(R_{i,p})) \cdot d(x)
$$

To do this, we can use kernel density estimation ($KDE$) to approximate the probability densities of the samples of the risk scores: ($R_{i,p}^{(s)}$):

$$
\hat{f}(R_{i,p}) = KDE(R_{i,p}^{(s)})
$$

```{r}
# load the ks package
library(ks)

# approximate the probability using kernel density estimation
kde_list <- list()
for (i in seq_along(R_list)) {
  
  # fit the kernel density model
  kde_fit <- ks::kde(R_list[[i]])
  
  # evaluate density at each sample
  kde_list[[i]] <- predict(kde_fit, x = R_list[[i]])
  
}

# rename the list
names(kde_list) <- names(R_list)

```

We can then use the Monte Carlo approximation of the integral to calculate entropy:

$$
H(R_{i,p}) = -\frac{1}{S}\sum_{s = 1}^{S}log(\hat{f}(R_{i,p}))
$$

For example, if we want to calculate entropy for "site_5", we could do the following:

```{r}
# calculate entropy for "site 5"
-mean(log(kde_list[[5]]))
```

This will become important later when we design our sampling framework to maximise information gain. In addition to entropy, we can calculate the expected value of the risk score in a given grid-cell:

$$
\mathbb{E}_{R_{i,p}}[R_{i,p}] = \frac{1}{n} \times \sum(R_{i,p} = r_{i,p})
$$

From these expected values for each grid-cell, we can calculate the overall risk score for the focal protected area (i.e. "pa_1") as:

$$
R^{tot}_{p} = \sum \mathbb{E}_{R_{i,p}}[R_{i,p}]
$$

For example, we could do the following to calculate the overall risk score of the grid

```{r}
# calculate the overall risk score for pa_1
sum(sapply(R_list, mean))
```

This framework and the methods discussed so far provide a baseline for how we will quantify the probability of occurrence of each species for a given protected area and also the way that we will apply information theory concepts.

### Information-theory approach

The next step in the **AIRIES** workflow is to choose sites to sample for eDNA that will maximise information about the risk of invasive species to protected areas. To do this, we will simulate the change in the risk score distribution for a given grid cell $i$ to protected area $p$ after eDNA sampling (i.e. detection or non-detection): Going from: $p(R_{i,p})_{\text{before}}$ to $p(R_{i,p})_{\text{after}}$.

Based on this change in distribution, we will assess two aspects:

#### Information gain

How much information would we gain by conducting eDNA sampling in grid-cell $i$ in terms of the risk to protected area $p$? For this, we would compare $p(R_{i,p,\text{before}})$ to $p(R_{i,p,\text{after}})$ using the Kullback-Leibler (KL) divergence.

This is a measure of how information this data has added to our knowledge about the risk of the species to protected area $p$ in grid-cell $i$.

$$
\hat{f}(R_{i,p,\text{before}}^{(s)}) = KDE(R_{i,p,\text{before}}^{(s)})
$$

$$
\hat{f}(R_{i,p,\text{after}}^{(s)}) = KDE(R_{i,p,\text{after}}^{(s)})
$$

We can then use these kernel density estimates to compute KL divergence using Monte-Carlo methods:

$$
D_{KL} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,\text{after}}^{(s)})}{\hat{f}(R_{i,p,\text{before}}^{(s)})}\biggr]
$$

#### Change in risk

This is still under development...

Information about risk is important. However, we also want to asymmetrically value risk. Imagine we have grid-cell $i$ which, based on current information, has low risk to protected area $p$. Let's say a detection of the species would cause a small relative decrease in risk but a large relative 

We simulate taking an eDNA sample

If taking an eDNA sample (i.e. detection or non-detection) will increase the risk from grid-cell $i$ to protected area $p$, then this is more valuable to know than if the risk will decrease. An increase in risk could demand action, a decrease in risk is good to know but does not necessarily cause action.

#### Decision theory

Calculating KL-divergence and $\Delta \text{risk}$ in each grid-cell, we can produce a map of where sampling would lead to the greatest information gain and change in average risk. Based on this map, we can use optimisation algorithms to then prioritise sites.

### Observation model

The idea behind **AIRIES** is to update our prior risk distribution ($p(R_{i,p})_{\text{before}}$) based on taking an eDNA sample and testing for a species: $p(R_{i,p})_{\text{after}}$. Therefore, we need a way to update our risk score. Our risk score equation is:

$$
R_{i,p}^{(s)} = \psi_i^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Therefore, to update the distribution, we simply need to update the $psi_i$ values as the rest are constants:

$$
R_{i,p,updated}^{(s)} = \psi_{i, updated}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

To do this, we need an **observation model** of how eDNA detections ($y_{ij}$) in sample $j$ and in grid-cell $i$ translate to the probability of detection ($p_{i}$). We use the following observation model:

$$
y_{ij} \sim Bernoulli(p_{i})
$$ 

$$
p_{i} = z_i \times (1 - \theta^{FN}_{field}) \times (1 - \theta^{FN}_{lab}) + (1 - z_i) \times \theta^{FP}_{lab}
$$ 

Where:

$z_i$: true presence (1) or true absence (0)

$\theta^{FN}_{field}$ false negative rate in the field (i.e. non-detection when species is present). Therefore, 1 - $\theta^{FN}_{field}$ is the true positive rate in the field (i.e. the probability that a species is detected when it is present).

$\theta^{FN}_{lab}$ false negative rate in the lab (i.e. non-detection when species is present in the sample). Therefore, 1 - $\theta^{FN}_{lab}$ is the true positive rate in the lab (i.e. the probability that a species is detected following PCR when its DNA is present).

$\theta^{FP}_{lab}$ false positive rate in the lab (i.e. detection when species is absent from the sample which can occur via, for example, contamination).

**Bayesian updating**

With this as our underlying observational process model, we can use Bayesian updating to determine how the occupancy probability and, therefore, the risk score would change if we collected an airborne eDNA sample for a given grid-cell. Specifically, consider grid-cell $i$ and the let the prior probability that species A is present be $psi_{i,prior}$.

Now, assume that we sample eDNA in grid-cell $i$ and obtain a positive detection for species A (i.e. $y_i = 1$). We calculate the posterior probability that species A is present (i.e. $z_i = 1$) in grid-cell $i$ using Bayes rule:

$$
P(z_i=1|y_i = 1) = \frac{p_{11} \psi_{i,prior}}{p_{11} \psi_{i,prior} + p_{01}(1-\psi_{i,prior})}
$$

Where:

$z_i \epsilon [0,1]$: true presence (1) or absence (0) of species A in grid-cell $i$ $y_i \epsilon [0,1]$: detection (1) or non-detection (0) of species A in grid-cell $i$

$\psi_{i,prior} = P(z_i = 1)$: prior probability that species A is present in grid-cell $i$

$p_{11} = P(y_i = 1|z_i = 1)P(z_i = 1) = (1 - \theta^{FN}_{field}) \cdot (1 - \theta^{FN}_{lab})$: true positive rate derived from the probability of false negatives in the field and lab.

$p_{01} = P(y_i = 1|z_i = 0)P(z_i = 0) = \theta^{FP}_{lab}$: false positive rate derived from the probability of false positives in the lab

However, there is also the possibility that the eDNA sampling results in non-detection (i.e. $y_i = 0$). We need to account for both possibilities. For this, we need to calculate the posterior probability that species A is present (i.e. $z_i = 1$) in grid-cell $s$ given non-detection (i.e. $y_i = 0$):

$$
P(z_s = 1|y_s = 0) = \frac{(1 - p_{11}) \psi_{i,prior}}{(1 - p_{11}) \psi_{i,prior} + (1 - p_{01})(1-\psi_{i,prior})}
$$

Now, as an example, we will perform the updating calculations for grid-cell $i$ (i.e. loop over each cell) when there is a detection ($y_i = 1$) and when there is a non-detection ($y_i = 0$). To do this, we assume that we have samples from our observation model for:

$p_{11} = P(y_i = 1|z_i = 1)P(z_i = 1) = (1 - \theta^{FN}_{field}) \cdot (1 - \theta^{FN}_{lab})$: true positive rate derived from the probability of false negatives in the field and lab.

$p_{01} = P(y_i = 1|z_i = 0)P(z_i = 0) = \theta^{FP}_{lab}$: false positive rate derived from the probability of false positives in the lab

Practically, these samples (i.e. $p_{11}^{s}$ and $p_{01}^{s}$) would come from fitting the **observation model** to data for a given species based on experiments with and without known species. For now, we will simply assume we have samples for the $\theta^{FN}_{field}$, $\theta^{FN}_{lab}$ and $\theta^{FP}_{lab}$ parameters and then use these samples to derive the samples for $p_{11}^{s}$ and $p_{01}^{s}$.

For this, we use the Beta() distribution:

```{r}
# draw samples for $\theta^{FN}_{field}$, $\theta^{FN}_{lab}$ and $\theta^{FP}_{lab}$
kappa <- 25

# false negative rate in the field
theta_fn_field <- 
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.25, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.25, kappa = kappa)) |> 
  round(5)

# false negative rate in the lab
theta_fn_lab <-
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.05, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.05, kappa = kappa)) |> 
  round(5)

# false positive rate in the lab
theta_fp_lab <-
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.025, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.025, kappa = kappa)) |> 
  round(5)
```

Based on these distributions (which come from the same model and, therefore, can be combined sample by sample), we get samples for: $p_{11}^{s}$ and $p_{01}^{s}$

```{r}
# calculate the composite parameters
p11_s <- (1 - theta_fn_field) * (1 - theta_fn_lab)
p01_s <- (theta_fp_lab)
```

To perform the Bayesian updating, we use kernel density estimation to estimate the probability distributions from these samples (i.e. for $psi_{i,prior}^{(s)}$, $p_{11}^{s}$ and $p_{01}^{s}$. We then draw samples from these kernel density estimates and use these samples to estimate the posterior distribution of species occupancy given detection: $P(z_i=1|y_i = 1)$ and given non-detection $P(z_i=1|y_i = 0)$.

We also estimate the probability of detection as this will be required when estimating information gain in a later step:

$$
p_{det} = P(y_s = 1) = p_{11}\psi + p_{01}(1 - \psi)
$$

```{r}
# for each grid-cell

# prior probability
psi_prior_list <- list()

# probability after detection
psi_post_det_list <- list()

# probability after non-detection
psi_post_no_det_list <- list()

# probability of detection
p_det_list <- list()

# number of samples
n <- 100

for (i in seq_len(nrow(grid))) {
  
  # estimate the kernel density functions for $psi_{i,prior}^{(s)}$, $p_{11}^{s}$ and $p_{01}^{s}$
  kde_psi_prior <- ks::kde(psi_list[[i]])
  kde_p11 <- ks::kde(p11_s)
  kde_p01 <- ks::kde(p01_s)
  
  # sample from these kdes
  psi_prior_samples <- ks::rkde(n = n, fhat = kde_psi_prior)
  p11_samples <- ks::rkde(n = n, fhat = kde_p11)
  p01_samples <- ks::rkde(n = n, fhat = kde_p01)
  
  # bound values between 0-1
  psi_prior_samples <- pmin(pmax(psi_prior_samples, 10e-9), 0.99999999999)
  p11_samples <- pmin(pmax(p11_samples, 10e-9), 0.99999999999)
  p01_samples <- pmin(pmax(p01_samples, 10e-9), 0.99999999999)
  
  # compute the posterior probabilities under detection
  num <- (p11_samples * psi_prior_samples)
  den <- (p11_samples * psi_prior_samples) + (p01_samples * (1 - psi_prior_samples))
  psi_post_det_list[[i]] <- num / den
  
  # compute posterior probabilities under non-detection
  num <- ((1 - p11_samples) * psi_prior_samples)
  den <- ((1 - p11_samples) * psi_prior_samples) + ((1 - p01_samples) * (1 - psi_prior_samples))
  psi_post_no_det_list[[i]] <- num / den
  
  # compute probability of detection
  p_det_list[[i]] <- (p11_samples * psi_prior_samples) + (p01_samples * (1 - psi_prior_samples))
 
  # save the prior-samples
  psi_prior_list[[i]] <- psi_prior_samples
   
}

# check the data
psi_post_det_list[[1]] |> round(5)

```

Using these simulated probabilities of occurrence for a given grid-cell $i$ under detection ($P(z_i=1|y_i = 1)$) and no detection ($P(z_i=1|y_i = 0)$) scenarios, we can calculated updated risk scores.

Assuming detection, we get the following:

$$
P(z_i=1|y_i = 1) \sim \psi_{i, updated: \text{ detection}}^{(s)}
$$

$$
R_{i,p,updated: \text{ detection}}^{(s)} = \psi_{i, updated: \text{ detection}}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Assuming non-detection, we get the following:

$$
P(z_i=1|y_i = 0) \sim \psi_{i, updated: \text{ non-detection}}^{(s)}
$$

$$
R_{i,p,updated: \text{ non-detection}}^{(s)} = \psi_{i, updated: \text{ non-detection}}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Here, we calculate the updated risk scores under scenarios of detection and no-detection:

```{r}
# detection: calculate the updated risk values for each cell
R_det_list <- list()
for (i in seq_len(nrow(grid))) {
  R_det_list[[i]] <- psi_post_det_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_det_list) <- grid$site

# non-detection: calculate the updated risk values for each cell
R_no_det_list <- list()
for (i in seq_len(nrow(grid))) {
  R_no_det_list[[i]] <- psi_post_no_det_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_no_det_list) <- grid$site
```


### Estimate information gain

With samples from the posterior distribution of the risk to protected area $p$ from grid-cell $i$ under scenarios of detection ($R_{i,p,updated: \text{ detection}}^{(s)} $) and non-detection ($R_{i,p,updated: \text{ non-detection}}^{(s)}$), we can compute the KL-divergence under both these scenarios not that $\hat{f}$ denotes the kernel density estimated distribution:

$$
D_{KL, \text{ detection}} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,updated: \text{ detection}}^{(s)})}{\hat{f}(R_{i,p,before}^{(s)})}\biggr]
$$

$$
D_{KL, \text{ non-detection}} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,updated: \text{ non-detection}}^{(s)})}{\hat{f}(R_{i,p,before}^{(s)})}\biggr]
$$

The **expected information gain** in grid-cell $i$ to protected area $p$ ($EIG_i,p$) from sampling is then a weighted average of these two KL-divergences which is weighted by the probability of detection ($p_{det}$). Because $p_{det}$ is a distribution of samples (i.e. $p_{det}^{(s)}$), this results in a distribution of $EIG_i,p$ values i.e. ($EIG_{i,p}^{s}$):

$$
EIG_{i,p}^{s} = p_{det}^{(s)}\cdot D_{KL, \text{ detection}}  + (1 - p_{det}^{(s)}) \cdot D_{KL, \text{ non-detection}}
$$

```{r}
# define function for the kl-divergence based on kde samples
kl_divergence <- function(q_samples, p_samples) {
  
  # KDE fit using ks
  kde_post <- ks::kde(x = q_samples)
  kde_prior <- ks::kde(x = p_samples)
  
  # evaluate both densities at the posterior sample points
  q_vals <- predict(kde_post, x = q_samples)
  p_vals <- predict(kde_prior, x = q_samples)
  
  # avoid division by zero / log(0)
  q_vals <- pmax(q_vals, 1e-10)
  p_vals <- pmax(p_vals, 1e-10)
  
  # monte Carlo estimate of KL divergence
  mean(log(q_vals / p_vals))

}

# calculate eig for each grid cell weighted by probability of detection
eig_list <- list()
for (i in seq_along(psi_prior_list)) {

  # kl_divergence detection
  kl_det <- kl_divergence(q_samples = psi_post_det_list[[i]], 
                          p_samples = psi_prior_list[[i]])
  
  # kl_divergence non-detection
  kl_no_det <- kl_divergence(q_samples = psi_post_no_det_list[[i]], 
                             p_samples = psi_prior_list[[i]])
  
  # get the eig for each grid cell distribution
  eig <- (p_det_list[[i]] * kl_det) + ((1 - p_det_list[[i]]) * kl_no_det)
  
  # wrap into a data.frame
  eig_list[[i]] <- dplyr::tibble(site = grid$site[i],
                                 sample = seq_along(eig),
                                 prior_psi = psi_prior_list[[i]],
                                 post_det_psi = psi_post_det_list[[i]],
                                 post_no_det_psi = psi_post_no_det_list[[i]],
                                 R_initial = R_list[[i]],
                                 R_after_detection = R_det_list[[i]],
                                 R_after_no_detection = R_no_det_list[[i]],
                                 eig_value = eig)
  
}
```

Plot the **expected information gain** of sampling in the different grid-cells relative to our theoretical protected area:

```{r}
# load the plotting function
library(ggplot2)
library(patchwork)

# bind into a data.frame and join
plot_dat <-
  dplyr::left_join(grid, dplyr::bind_rows(eig_list), by = "site")

# plot prior risk
p1 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = R_initial)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Prior risk") +
  xlab("One-dimensional location")

# plot risk after sampling: Detection
p2 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = R_after_detection)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Posterior risk (detection)") +
  xlab("One-dimensional location")

# plot risk after sampling: Non-detection
p3 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = R_after_no_detection)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Posterior risk (no detection)") +
  xlab("One-dimensional location")

# plot EIG
p4 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = eig_value)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Expected Information Gain (nats)") +
  xlab("One-dimensional location") 

p1234 <- (p1 + p2) / (p3 + p4) & theme_bw()
p1234
```

```{r}
# plot prior probability of occurrence
p1 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = prior_psi)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Prior probability of occurrence") +
  xlab("One-dimensional location")

p2 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = post_det_psi)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Posterior probability of occurrence (detection)") +
  xlab("One-dimensional location")

p3 <-
  ggplot(data = plot_dat,
       mapping = aes(x = spatial_location, y = post_no_det_psi)) +
  ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05) +
  ylab("Posterior probability of occurrence (non-detection)") +
  xlab("One-dimensional location")

p1 + p2 + p3 & theme_bw()
```





**More protected areas and more species**

This example is based on one species and one protected area. However, the idea behind **AIRIAS** is to do this for more than 40 different invasive alien species across the Natura 2000 network of Belgium. Therefore, for each grid cell, for each species and each protected area, there will be some EVS value. Our proposed sampling scheme will then need to prioritise sites that provide maximum information value across species and protected areas given limited resources.

Therefore, on top of this risk EVS model, we need a prioritisation algorithm that attempts to balance EVS gains for different protected areas and species.

To simulate this, we will simply assign random EVS values to sites for different species and protected areas for testing and developing the algorithm.

```{r}
# simulate a full EVS dataset across protected areas and species

# number of species
n_sp <- 10

# number of protected areas
n_pa <- 5

# number of sites
n_sites <- 100

# simulate datasets
opt_grid <-
  tidyr::expand_grid(species = paste0("sp_", seq_len(n_sp)),
                     pa = paste0("pa_", seq_len(n_pa)),
                     site = paste0("site_", seq_len(n_sites))) |>
  dplyr::group_by(species, pa) |>
  dplyr::mutate(EVS = runif(n = n_sites, 0.001, 0.1)) |>
  dplyr::ungroup()

# check the data
head(opt_grid)
```

Let's say that we can only sample 10 sites. Our goal is to protect all the different protected areas and maximise the EVS across species. The simplest way to do this is using a *greedy* approach where the EVS values are simply across protected areas and species for each site. We then rank the sites. This approach has no constraints and simply selects sites that give the most information across protected areas and species.

```{r}
# sum EVS across species and, per site:
site_scores <- 
  opt_grid |>
  dplyr::group_by(site) |>
  dplyr::summarise(EVS_total = sum(EVS)) |>
  dplyr::arrange(dplyr::desc(EVS_total))

# pick the top 10 sites:
selected_sites <- site_scores |> dplyr::slice_head(n = 10)

# check the 10 selected sites
selected_sites
```

A second approach would be to add constraints such as covering each protected area (i.e. EVS values computed from the perspective of that protected area).
