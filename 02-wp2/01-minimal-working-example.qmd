---
title: "minimal-working-example"
format: html
editor: source
---

### Basic approach

This is a minimal working example for how the **AIRIES** (*Adaptive Invasion-Risk Intelligence for Protected Areas with eDNA Surveillance*) project is envisioned to work.

Consider a single protected area (e.g. "pa_1") and a single invasive species ("sp_1"). Moreover, let's consider a simple one-dimensional landscape with N grid-cells or "sites" where some sites (e.g. "site_1", "site_2" and "site_3") form the protected area (i.e. "pa_1"):

```{r}
# number of locations
n_loc <- 100

# number of pa_cells
n_pa <- 5

# create a 1-dimension landscape
grid <- dplyr::tibble(spatial_location = runif(n = n_loc, 0, 10))

# arrange by spatial location
grid <- dplyr::arrange(grid, spatial_location)

# add a site name
grid$site <- paste0("site_", 1:n_loc)

# add protected area identity
grid$pa <- "pa_1"

# add a variable with pa_in
grid$pa_in <- c(rep(1, n_pa), rep(0, n_loc - n_pa))

# arrange columns
grid <- dplyr::select(grid, site, spatial_location, pa, pa_in)

# check these data
head(grid)
```

This data.frame shows N sites at a numeric spatial location for "pa_1" and shows that some sites are within "pa_1" (i.e. pa_in == 1). The data is set-up in this way so that it is easy to estimate the influence of all grid-cells on a given protected area.

Next, we define a simple dispersal kernel based on how far each grid-cell is from the focal protected area (i.e. "pa_1"). For this, we first calculate the distance between each site and the centroid of the focal protected area (in one-dimension, the centroid is simply the mean). Grid-cells that comprise the protected area (i.e. "site_1", "site_2" and "site_3") are given distances from the focal protected area of 0.

```{r}
# calculate distance
grid$dist_pa <- abs(grid$spatial_location - mean(grid$spatial_location[grid$pa_in == 1]))
grid$dist_pa <- with(grid, ifelse(pa_in == 1, 0, spatial_location))

# check the data
head(grid)
```

Using this distance, we define a kernel weight $K(i, p) \epsilon [0, 1]$ which describes how dispersal from grid-cell $i$ contributs to protected area $p$ if grid-cell $i$ is within protected area $p$ (otherwise, a value of 1 is given for $K(i, p)$):

$$
K(i, p) = exp(-d(i, p)/\alpha)
$$

```{r}
# define dispersal distance alpha
alpha <- 1.5

# implement the dispersal kernel
grid$k_ip <- with(grid,
                  ifelse(dist_pa == 0, 1, exp(-(dist_pa/alpha))))

# check the data
head(grid)
```

We are evalusating risk to the PA from cells outside the PA. Therefore, we remove the PAs and, therefore, conduct the rest of the analysis for grid-cells not present in the PA:

```{r}
# keep a copy of the PA data for plotting
grid_pa <- dplyr::filter(grid, pa_in == 1)

# extract sites not present in the pa from the grid
grid <- dplyr::filter(grid, pa_in == 0)
```

The next piece of information is the prior distribution that grid-cell $i$ is occupied by the focal species. We would typically derive this using a logistic-regression model of known presence-absence data (i.e. a kind of species distribution model) fit in a Bayesian framework where $y_i$ is the presence (1) or absence (0) of the species in grid-cell i. Therefore, we would approximate the posterior probability distribution $p(\psi_i | y_i)$ using posterior samples ($\psi_i^{(s)} \text{ where }s = 1,...,S$). Note that while this is technically a posterior distribution, we will use it as the **prior distribution** of species occupancy for each grid-cell.

To simulate this, we draw the samples ($\psi_i^{(s)} \text{ where }s = 1,...,S$) from a Beta() distribution. The mean of a beta distribution is given by a mean ($\mu$) and a precision parameter ($\kappa$):

$$
\mu = \frac{\alpha}{\alpha + \beta}
$$

$$
\kappa = \alpha + \beta
$$

The function in R `rbeta` is parameterised unsing the $\alpha$ and $\beta$ parameters. Therefore, to parameterise them in terms of the ($\mu$) and ($\kappa$), we use the following formulae:

$$
\alpha = \mu + \kappa
$$

$$
\beta = \kappa \times(1 - \mu)
$$

Therefore, we can define a formula to convert the $\mu$ and $\kappa$ values to $\alpha$ and $\beta$ values which makes it easier to parameterise the Beta() distribution.

```{r}
# define the function
rbeta_parameters <- function(parameter, mu, kappa) {
  if (parameter == "alpha") {
    mu * kappa
  } else if (parameter == "beta") {
    kappa * (1 - mu)
  } else {
    stop("choose an appropriate parameter argument value")
  }
}
```

```{r}
# set these probabilities manually and use a beta-distribution
mu <- seq(0.05, 0.25, length.out = nrow(grid))
kappa <- 25

# generate the psi-values
psi_list <-
  lapply(mu, function(x) {
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = x, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = x, kappa = kappa)) |> 
      round(5)
})

# name the list by site
names(psi_list) <- grid$site
```

For each grid-cell and each sample of species occupancy ($\psi_i^{(s)} \text{ where }s = 1,...,S$), we define each grid-cell's contribution to the risk to the focal protected area ("pa_1" in this case) as:

$$
R_{i,p}^{(s)} = \psi_i^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

$$
\text{where: } \psi_i^{(s)} \text{ where }s = 1,...,S
$$

Where:

$\psi_i^{(s)}$ is the probability that a focal species is present in cell $i$ given sample $s$ \\ $w_{species}$ is a weight describing the potential ecological damage of the species \\ $w_{site}$ is a weight describing the ecological value of the protected area \\ $K_{i,p}$ is the dispersal kernel

```{r}
# set the species weight
w_species <- 0.5

# set the site weight
w_site <- 0.8

# calculate the risk values for each cell
R_list <- list()
for (i in seq_len(nrow(grid))) {
  R_list[[i]] <- psi_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_list) <- grid$site

# check the output
R_list[[1]]
```

This leaves us with estimates for two probability distributions for each grid-cell $i$ based on the samples from the posterior distribution:

$$
p(\psi_i) \sim \psi_i^{(s)} \text{ where }s = 1,...,S
$$

$$
p(R_{i,p}) \sim  R_{i,p}^{(s)} 
$$

Because these are valid probability distributions that represent uncertainty, we can subject them to information-theoretical concepts. For example, we can calculate the entropy associated with the risk ($R_{i,p}$) or occupancy ($\psi_i$) of a given grid-cell $i$ in protected area $p$. For example:

$$
H(R_{i,p}) = -\int p(R_{i,p}) \cdot log(p(R_{i,p})) \cdot d(x)
$$

To do this, we can use kernel density estimation ($KDE$) to approximate the probability densities of the samples of the risk scores: ($R_{i,p}^{(s)}$):

$$
\hat{f}(R_{i,p}) = KDE(R_{i,p}^{(s)})
$$

```{r}
# load the ks package
library(ks)

# approximate the probability using kernel density estimation
kde_list <- list()
for (i in seq_along(R_list)) {
  
  # fit the kernel density model
  kde_fit <- ks::kde(R_list[[i]])
  
  # evaluate density at each sample
  kde_list[[i]] <- predict(kde_fit, x = R_list[[i]])
  
}

# rename the list
names(kde_list) <- names(R_list)
```

We can then use the Monte Carlo approximation of the integral to calculate entropy:

$$
H(R_{i,p}) = -\frac{1}{S}\sum_{s = 1}^{S}log(\hat{f}(R_{i,p}))
$$

For example, if we want to calculate entropy for "site_5", we could do the following:

```{r}
# calculate entropy for "site 5"
-mean(log(kde_list[[5]]))
```

This will become important later when we design our sampling framework to maximise information gain. In addition to entropy, we can calculate the expected value of the risk score in a given grid-cell:

$$
\mathbb{E}_{R_{i,p}}[R_{i,p}] = \frac{1}{n} \times \sum(R_{i,p} = r_{i,p})
$$

For example, we could calculate the expected risk score from each grid cell to the protected area:

```{r}
# calculate the overall risk score for pa_1
sapply(R_list, mean)
```

This framework and the methods discussed so far provide a baseline for how we will quantify the probability of occurrence of each species for a given protected area and also the way that we will apply information theory concepts.

### Information-theory approach

The next step in the **AIRIES** workflow is to choose sites to sample for eDNA that will maximise information about the risk of invasive species to protected areas. To do this, we will simulate the change in the risk score distribution for a given grid cell $i$ to protected area $p$ after eDNA sampling (i.e. detection or non-detection): Going from: $p(R_{i,p})_{\text{before}}$ to $p(R_{i,p})_{\text{after}}$.

Based on this change in distribution, we will assess two aspects:

#### Information gain

How much information would we gain by conducting eDNA sampling in grid-cell $i$ in terms of the risk to protected area $p$? For this, we would compare $p(R_{i,p,\text{before}})$ to $p(R_{i,p,\text{after}})$ using the Kullback-Leibler (KL) divergence.

This is a measure of how information this data has added to our knowledge about the risk of the species to protected area $p$ in grid-cell $i$.

$$
\hat{f}(R_{i,p,\text{before}}^{(s)}) = KDE(R_{i,p,\text{before}}^{(s)})
$$

$$
\hat{f}(R_{i,p,\text{after}}^{(s)}) = KDE(R_{i,p,\text{after}}^{(s)})
$$

We can then use these kernel density estimates to compute KL divergence using Monte-Carlo methods:

$$
D_{KL} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,\text{after}}^{(s)})}{\hat{f}(R_{i,p,\text{before}}^{(s)})}\biggr]
$$

#### Decision theory

Calculating KL-divergence in each grid-cell, we can produce a map of where sampling would lead to the greatest information gain. Based on this map, we can use optimisation algorithms to then prioritise sites.

### Observation model

The idea behind **AIRIES** is to update our prior risk distribution ($p(R_{i,p})_{\text{before}}$) based on taking an eDNA sample and testing for a species: $p(R_{i,p})_{\text{after}}$. Therefore, we need a way to update our risk score. Our risk score equation is:

$$
R_{i,p}^{(s)} = \psi_i^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Therefore, to update the distribution, we simply need to update the $psi_i$ values as the other parameters do not change for a given grid-cell, species and protected area combination:

$$
R_{i,p,updated}^{(s)} = \psi_{i, updated}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

To do this, we need an **observation model** of how eDNA detections ($y_{ij}$) in sample $j$ and in grid-cell $i$ translate to the probability of detection ($p_{i}$). We use the following observation model:

$$
y_{ij} \sim Bernoulli(p_{i})
$$

$$
p_{i} = z_i \times (1 - \theta^{FN}_{field}) \times (1 - \theta^{FN}_{lab}) + (1 - z_i) \times \theta^{FP}_{lab}
$$

Where:

$z_i$: true presence (1) or true absence (0)

$\theta^{FN}_{field}$ false negative rate in the field (i.e. non-detection when species is present). Therefore, 1 - $\theta^{FN}_{field}$ is the true positive rate in the field (i.e. the probability that a species is detected when it is present).

$\theta^{FN}_{lab}$ false negative rate in the lab (i.e. non-detection when species is present in the sample). Therefore, 1 - $\theta^{FN}_{lab}$ is the true positive rate in the lab (i.e. the probability that a species is detected following PCR when its DNA is present).

$\theta^{FP}_{lab}$ false positive rate in the lab (i.e. detection when species is absent from the sample which can occur via, for example, contamination).

**Bayesian updating**

With this as our underlying observational process model, we can use Bayesian updating to determine how the occupancy probability and, therefore, the risk score would change if we collected an airborne eDNA sample for a given grid-cell. Specifically, consider grid-cell $i$ and the let the prior probability that species A is present be $psi_{i,prior}$.

Now, assume that we sample eDNA in grid-cell $i$ and obtain a positive detection for species A (i.e. $y_i = 1$). We calculate the posterior probability that species A is present (i.e. $z_i = 1$) in grid-cell $i$ using Bayes rule:

$$
P(z_i=1|y_i = 1) = \frac{p_{11} \psi_{i,prior}}{p_{11} \psi_{i,prior} + p_{01}(1-\psi_{i,prior})}
$$

Where:

$z_i \epsilon [0,1]$: true presence (1) or absence (0) of species A in grid-cell $i$ $y_i \epsilon [0,1]$: detection (1) or non-detection (0) of species A in grid-cell $i$

$\psi_{i,prior} = P(z_i = 1)$: prior probability that species A is present in grid-cell $i$

$p_{11} = P(y_i = 1|z_i = 1)P(z_i = 1) = (1 - \theta^{FN}_{field}) \cdot (1 - \theta^{FN}_{lab})$: true positive rate derived from the probability of false negatives in the field and lab.

$p_{01} = P(y_i = 1|z_i = 0)P(z_i = 0) = \theta^{FP}_{lab}$: false positive rate derived from the probability of false positives in the lab

However, there is also the possibility that the eDNA sampling results in non-detection (i.e. $y_i = 0$). We need to account for both possibilities. For this, we need to calculate the posterior probability that species A is present (i.e. $z_i = 1$) in grid-cell $s$ given non-detection (i.e. $y_i = 0$):

$$
P(z_s = 1|y_s = 0) = \frac{(1 - p_{11}) \psi_{i,prior}}{(1 - p_{11}) \psi_{i,prior} + (1 - p_{01})(1-\psi_{i,prior})}
$$

Now, as an example, we will perform the updating calculations for grid-cell $i$ (i.e. loop over each cell) when there is a detection ($y_i = 1$) and when there is a non-detection ($y_i = 0$). To do this, we assume that we have samples from our observation model for:

$p_{11} = P(y_i = 1|z_i = 1)P(z_i = 1) = (1 - \theta^{FN}_{field}) \cdot (1 - \theta^{FN}_{lab})$: true positive rate derived from the probability of false negatives in the field and lab.

$p_{01} = P(y_i = 1|z_i = 0)P(z_i = 0) = \theta^{FP}_{lab}$: false positive rate derived from the probability of false positives in the lab

Practically, these samples (i.e. $p_{11}^{s}$ and $p_{01}^{s}$) would come from fitting the **observation model** to data for a given species based on experiments with and without known species. For now, we will simply assume we have samples for the $\theta^{FN}_{field}$, $\theta^{FN}_{lab}$ and $\theta^{FP}_{lab}$ parameters and then use these samples to derive the samples for $p_{11}^{s}$ and $p_{01}^{s}$.

For this, we use the Beta() distribution:

```{r}
# draw samples for $\theta^{FN}_{field}$, $\theta^{FN}_{lab}$ and $\theta^{FP}_{lab}$
kappa <- 25

# false negative rate in the field
theta_fn_field <- 
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.25, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.25, kappa = kappa)) |> 
  round(5)

# false negative rate in the lab
theta_fn_lab <-
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.05, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.05, kappa = kappa)) |> 
  round(5)

# false positive rate in the lab
theta_fp_lab <-
  rbeta(n = 100, 
        shape1 = rbeta_parameters("alpha", mu = 0.025, kappa = kappa), 
        shape2 = rbeta_parameters("beta", mu = 0.025, kappa = kappa)) |> 
  round(5)
```

Based on these distributions (which come from the same model and, therefore, can be combined sample by sample), we get samples for: $p_{11}^{s}$ and $p_{01}^{s}$

```{r}
# calculate the composite parameters
p11_s <- (1 - theta_fn_field) * (1 - theta_fn_lab)
p01_s <- (theta_fp_lab)
```

To perform the Bayesian updating, we use kernel density estimation to estimate the probability distributions from these samples (i.e. for $psi_{i,prior}^{(s)}$, $p_{11}^{s}$ and $p_{01}^{s}$. We then draw samples from these kernel density estimates and use these samples to estimate the posterior distribution of species occupancy given detection: $P(z_i=1|y_i = 1)$ and given non-detection $P(z_i=1|y_i = 0)$.

We also estimate the probability of detection as this will be required when estimating information gain in a later step:

$$
p_{det} = P(y_s = 1) = p_{11}\psi + p_{01}(1 - \psi)
$$

```{r}
# for each grid-cell

# prior probability
psi_prior_list <- list()

# probability after detection
psi_post_det_list <- list()

# probability after non-detection
psi_post_no_det_list <- list()

# probability of detection
p_det_list <- list()

# number of samples
n <- 100

for (i in seq_len(nrow(grid))) {
  
  # estimate the kernel density functions for $psi_{i,prior}^{(s)}$, $p_{11}^{s}$ and $p_{01}^{s}$
  kde_psi_prior <- ks::kde(psi_list[[i]])
  kde_p11 <- ks::kde(p11_s)
  kde_p01 <- ks::kde(p01_s)
  
  # sample from these kdes
  psi_prior_samples <- ks::rkde(n = n, fhat = kde_psi_prior)
  p11_samples <- ks::rkde(n = n, fhat = kde_p11)
  p01_samples <- ks::rkde(n = n, fhat = kde_p01)
  
  # bound values between 0-1
  psi_prior_samples <- pmin(pmax(psi_prior_samples, 10e-9), 0.99999999999)
  p11_samples <- pmin(pmax(p11_samples, 10e-9), 0.99999999999)
  p01_samples <- pmin(pmax(p01_samples, 10e-9), 0.99999999999)
  
  # compute the posterior probabilities under detection
  num <- (p11_samples * psi_prior_samples)
  den <- (p11_samples * psi_prior_samples) + (p01_samples * (1 - psi_prior_samples))
  psi_post_det_list[[i]] <- num / den
  
  # compute posterior probabilities under non-detection
  num <- ((1 - p11_samples) * psi_prior_samples)
  den <- ((1 - p11_samples) * psi_prior_samples) + ((1 - p01_samples) * (1 - psi_prior_samples))
  psi_post_no_det_list[[i]] <- num / den
  
  # compute probability of detection
  p_det_list[[i]] <- (p11_samples * psi_prior_samples) + (p01_samples * (1 - psi_prior_samples))
 
  # save the prior-samples
  psi_prior_list[[i]] <- psi_prior_samples
   
}

# check the data
psi_post_det_list[[1]] |> round(5)

```

Using these simulated probabilities of occurrence for a given grid-cell $i$ under detection ($P(z_i=1|y_i = 1)$) and no detection ($P(z_i=1|y_i = 0)$) scenarios, we can calculated updated risk scores.

Assuming detection, we get the following:

$$
P(z_i=1|y_i = 1) \sim \psi_{i, updated: \text{ detection}}^{(s)}
$$

$$
R_{i,p,updated: \text{ detection}}^{(s)} = \psi_{i, updated: \text{ detection}}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Assuming non-detection, we get the following:

$$
P(z_i=1|y_i = 0) \sim \psi_{i, updated: \text{ non-detection}}^{(s)}
$$

$$
R_{i,p,updated: \text{ non-detection}}^{(s)} = \psi_{i, updated: \text{ non-detection}}^{(s)} \cdot w_{species} \cdot w_{site} \cdot K_{i,p}
$$

Here, we calculate the updated risk scores under scenarios of detection and no-detection:

```{r}
# detection: calculate the updated risk values for each cell
R_det_list <- list()
for (i in seq_len(nrow(grid))) {
  R_det_list[[i]] <- psi_post_det_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_det_list) <- grid$site

# non-detection: calculate the updated risk values for each cell
R_no_det_list <- list()
for (i in seq_len(nrow(grid))) {
  R_no_det_list[[i]] <- psi_post_no_det_list[[i]] * w_species * w_site * grid$k_ip[i]
}
names(R_no_det_list) <- grid$site
```

### Estimate information gain

With samples from the posterior distribution of the risk to protected area $p$ from grid-cell $i$ under scenarios of detection (\$R\_{i,p,updated: \text{ detection}}\^{(s)} $) and non-detection ($R\_{i,p,updated: \text{ non-detection}}\^{(s)}\$), we can compute the KL-divergence under both these scenarios not that $\hat{f}$ denotes the kernel density estimated distribution:

$$
D_{KL, \text{ detection}} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,updated: \text{ detection}}^{(s)})}{\hat{f}(R_{i,p,before}^{(s)})}\biggr]
$$

$$
D_{KL, \text{ non-detection}} = \mathbb{E}\biggl[log\frac{\hat{f}(R_{i,p,updated: \text{ non-detection}}^{(s)})}{\hat{f}(R_{i,p,before}^{(s)})}\biggr]
$$

The **expected information gain** in grid-cell $i$ to protected area $p$ ($EIG_i,p$) from sampling is then a weighted average of these two KL-divergences which is weighted by the probability of detection ($p_{det}$). Because $p_{det}$ is a distribution of samples (i.e. $p_{det}^{(s)}$), this results in a distribution of $EIG_i,p$ values i.e. ($EIG_{i,p}^{s}$):

$$
EIG_{i,p}^{s} = p_{det}^{(s)}\cdot D_{KL, \text{ detection}}  + (1 - p_{det}^{(s)}) \cdot D_{KL, \text{ non-detection}}
$$

```{r}
# define function for the kl-divergence based on kde samples
kl_divergence <- function(q_samples, p_samples) {
  
  # KDE fit using ks
  kde_post <- ks::kde(x = q_samples)
  kde_prior <- ks::kde(x = p_samples)
  
  # evaluate both densities at the posterior sample points
  q_vals <- predict(kde_post, x = q_samples)
  p_vals <- predict(kde_prior, x = q_samples)
  
  # avoid division by zero / log(0)
  q_vals <- pmax(q_vals, 1e-10)
  p_vals <- pmax(p_vals, 1e-10)
  
  # monte Carlo estimate of KL divergence
  mean(log(q_vals / p_vals))

}

# calculate eig for each grid cell weighted by probability of detection
eig_list <- list()
for (i in seq_along(psi_prior_list)) {

  # kl_divergence detection
  kl_det <- kl_divergence(q_samples = psi_post_det_list[[i]], 
                          p_samples = psi_prior_list[[i]])
  
  # kl_divergence non-detection
  kl_no_det <- kl_divergence(q_samples = psi_post_no_det_list[[i]], 
                             p_samples = psi_prior_list[[i]])
  
  # get the eig for each grid cell distribution
  eig <- (p_det_list[[i]] * kl_det) + ((1 - p_det_list[[i]]) * kl_no_det)
  
  # wrap into a data.frame
  eig_list[[i]] <- dplyr::tibble(site = grid$site[i],
                                 sample = seq_along(eig),
                                 prior_psi = psi_prior_list[[i]],
                                 p_det = p_det_list[[i]],
                                 post_det_psi = psi_post_det_list[[i]],
                                 post_no_det_psi = psi_post_no_det_list[[i]],
                                 R_initial = R_list[[i]],
                                 R_after_detection = R_det_list[[i]],
                                 R_after_no_detection = R_no_det_list[[i]],
                                 eig_value = eig)
  
}
```

### Visualise the results

We can now visualise and interpret these simulations and what they mean for how our metrics of information inform our sampling. We start by visualising the prior probability of occurrence, the probability of detection and the posterior probabilities of occurrence in scenarios of detection and non-detection.

```{r}
# load the relevant plotting libraries
source(here::here("theme-meta.R"))
library(ggplot2)
library(patchwork)
library(rlang)

# create a data.frame for plotting
plot_dat <-
  dplyr::left_join(grid, dplyr::bind_rows(eig_list), by = "site")

# get the centroid of the protected area
pa_centroid <- mean(grid_pa$spatial_location)
```

```{r}
# function for plotting
px_func <- function(data, x, y, x_lab, y_lab, pa_location) {
  
  px <-
    ggplot(data = data,
           mapping = aes(x = !!sym(x), y = !!sym(y))) +
    ggbeeswarm::geom_quasirandom(shape = 1, size = 0.5, width = 0.05, alpha = 0.25) +
    ylab(y_lab) +
    xlab(x_lab) +
    geom_vline(xintercept = pa_location, colour = "red", linetype = "dashed")
  
  px
  
}

# prior psi
p1 <- px_func(data = plot_dat, x = "spatial_location", y = "prior_psi",
              x_lab = "One-dimensional location", y_lab = "Prior psi",
              pa_location = pa_centroid)

# probability of detection
p2 <- px_func(data = plot_dat, x = "spatial_location", y = "p_det",
              x_lab = "One-dimensional location", y_lab = "P(detection)",
              pa_location = pa_centroid)

# posterior probability given detection
p3 <- px_func(data = plot_dat, x = "spatial_location", y = "post_det_psi",
              x_lab = "One-dimensional location", y_lab = "Posterior psi (detection)",
              pa_location = pa_centroid)

# posterior probability given non-detection
p4 <- px_func(data = plot_dat, x = "spatial_location", y = "post_no_det_psi",
              x_lab = "One-dimensional location", y_lab = "Posterior psi (non-detection)",
              pa_location = pa_centroid)

(p1 + p2) / (p3 + p4) & theme_meta()

```

Plot the **expected information gain** of sampling in the different grid-cells relative to our theoretical protected area:

```{r}
# prior risk
p1 <- px_func(data = plot_dat, x = "spatial_location", y = "R_initial",
              x_lab = "One-dimensional location", y_lab = "Prior risk",
              pa_location = pa_centroid)

# probability of detection
p2 <- px_func(data = plot_dat, x = "spatial_location", y = "p_det",
              x_lab = "One-dimensional location", y_lab = "P(detection)",
              pa_location = pa_centroid)

# posterior risk: detection
p3 <- px_func(data = plot_dat, x = "spatial_location", y = "R_after_detection",
              x_lab = "One-dimensional location", y_lab = "Posterior risk (detection)",
              pa_location = pa_centroid)

# posterior risk: non-detection
p4 <- px_func(data = plot_dat, x = "spatial_location", y = "R_after_no_detection",
              x_lab = "One-dimensional location", y_lab = "Posterior risk (detection)",
              pa_location = pa_centroid)

# expected information gain
p5 <- px_func(data = plot_dat, x = "spatial_location", y = "eig_value",
              x_lab = "One-dimensional location", y_lab = "EIG",
              pa_location = pa_centroid)

p12345 <- (p1 + p2 + p3) / (p4 + p5) & theme_meta()
p12345
```

### Optimisation modelling

This example above is based on one species and one protected area. However, the idea behind **AIRIAS** is to do this for 50 different invasive alien species across the Natura 2000 network of Belgium. Therefore, for each grid cell, for each species and each protected area, there will be some **Expected Information Gain (EIG)** value. Our proposed sampling scheme will then need to prioritise sites that provide maximum information value across species and protected areas given limited resources.

Basically, there are a set of optimal sites based on KL-divergence and on absolute risk change for a given species and protected area. Want to use the optimisation approach to select the optimum sites for as many species and protected area combinations.

To simulate this, let's just assume that we have a distribution **Expected Information Gain (EIG)** values for all species and protected area combination. Then, let's say we take the mean EIG value for each grid-cell. For simplicity, we will draw these mean EIG values for each grid-cell and each species by protected combination from a truncated Normal() distribution:

```{r}
# select the number of species
n_sp <- 10

# select the number of protected areas
n_pa <- 5

# select the number of grid cells 
n_grid <- 100

# expand the grid
sim_grid <- tidyr::expand_grid(species = paste0("sp_", 1:n_sp),
                               protected_area = paste0("pa_", 1:n_pa),
                               grid_cell = 1:n_grid)

# draw mean EIG values for each grid cell for each species and each protected area
sim_grid$eig_value <- rnorm(n = nrow(sim_grid), mean = 6, sd = 3)
sim_grid$eig_value <- ifelse(sim_grid$eig_value <= 0, 0.05, sim_grid$eig_value)

# check the distribution
hist(sim_grid$eig_value)
```

Now that we have our simulated data, we can make the idea more precise. We want to identify an efficient eDNA sampling design that maximises information about the risk to Belgium's protected areas. For this, we use the `ompr` package. Our objective is to select a limited number of grid-cells to sample that, together, provide the greatested EIG across multiple species and protected areas.

Let:

-   ( G ): the set of candidate grid cells
-   ( C ): the set of all species--protected area combinations
-   ( P ): the set of protected areas
-   ( EIG\_{gc} ): the expected information gain (EIG) for grid cell ( g ) in combination ( c )

We define two sets of binary decision variables:

-   ( y_g = 1 ) if grid cell ( g \in G ) is selected; 0 otherwise\
-   ( z_c = 1 ) if species--PA combination ( c \in C ) is covered by at least one selected grid cell; 0 otherwise

Our objective is then to maximise:

\[ \text{Maximise } \sum\*{c\* \\in C} \sum{g \in G_c} EIG\_{gc} \cdot y_g \]

Where ( G_c \subseteq G ) is the subset of grid cells with top-ranked EIG for combination ( c ).\
This ensures that selected grid cells contribute the **highest possible cumulative information gain** across species and protected areas.

In addition, we set the following constraints to make sure that we sample in a way that makes sure we balance protected area coverage:

1.  **Sample size constraint**\
    Select exactly ( X ) grid cells (e.g. 20):

\[ \sum\_{g \in G} y_g = X \]

2.  **Combination coverage constraint**\
    A species--PA combination is considered "covered" if **at least one** of its associated top-EIG grid cells is selected:

\[ z_c \leq \sum\_{g \in G_c} y_g \quad \forall c \in C \]

3.  **Protected area coverage constraint**\
    At least one species--PA combination must be covered for **every protected area** ( p \in P ):

\[ \sum\_{c \in C_p} z_c \geq 1 \quad \forall p \in P \]

Where ( C_p \subseteq C ) is the set of combinations associated with protected area ( p ).

Given this optimisation framework, we can run the optimisation model and get the selected grid-cells:

```{r}
library(dplyr)
library(ompr)
library(ompr.roi)
library(ROI.plugin.glpk)

# get highest eig cells for each species x pa combination
eig10 <- 
  sim_grid |>
  dplyr::group_by(species, protected_area) |>
  dplyr::slice_max(eig_value, n = 10, with_ties = FALSE) |>
  dplyr::ungroup()

# create combination ID
eig10 <- 
  eig10 |>
  dplyr::mutate(comb_id = paste(species, protected_area, sep = "_"),
                grid_id = as.character(grid_cell))

# Index grid cells and combinations
grid_levels <- unique(eig10$grid_id)
comb_levels <- unique(eig10$comb_id)
pa_levels <- unique(eig10$protected_area)

eig10 <- 
  eig10 |>
  dplyr::mutate(grid_index = match(grid_id, grid_levels),
                comb_index = match(comb_id, comb_levels),
                pa_index = match(protected_area, pa_levels))

n_grid <- length(grid_levels)
n_comb <- length(comb_levels)
n_pa <- length(pa_levels)

# how many sites can be sampled?
sample_size <- 5

# combinations per PA (for the PA coverage constraint)
pa_to_combs <- split(eig10$comb_index, eig10$pa_index)

# build the model
model <- MIPModel() %>%
  # Decision variables
  add_variable(y[i], i = 1:n_grid, type = "binary") %>%
  add_variable(z[j], j = 1:n_comb, type = "binary") %>%

  # Objective: maximise weighted EIG
  set_objective(
    sum_expr(eig10$eig_value[k] * y[eig10$grid_index[k]], k = 1:nrow(eig10)),
    "max"
  ) %>%

  # Select exactly sample_size grid cells
  add_constraint(sum_expr(y[i], i = 1:n_grid) == sample_size) %>%

  # z[j] ≤ sum of y[i] for associated grid cells
  add_constraint(
    z[j] <= sum_expr(y[eig10$grid_index[k]], k = which(eig10$comb_index == j)),
    j = 1:n_comb
  ) %>%

  # For each PA, at least one of its z[j] is selected
  add_constraint(
    sum_expr(z[j], j = pa_to_combs[[as.character(p)]]) >= 1,
    p = names(pa_to_combs)
  )


# Solve model
result <- solve_model(model, with_ROI(solver = "glpk"))

# check the cells that were chosen
selected_cells <- 
  get_solution(result, y[i]) |>
  filter(value == 1) |>
  mutate(grid_id = grid_levels[i])

# check the covered combinations
covered_combinations <- 
  get_solution(result, z[j]) |>
  filter(value == 1) |>
  mutate(comb_id = comb_levels[j])

```

We can visualise which cells were chosen relative to random samples of grid-cells:

```{r}
# get all the data and label as all
dat_all <-
  eig10 |>
  dplyr::mutate(variable = "all") |>
  dplyr::select(grid_id, eig_value, variable)

# randomly sample N cells X separate times
X <- 5
random_sample <- 
  lapply(1:X, function(x) {
    sample(unique(dat_all$grid_id), size = sample_size)
  })

# get the optimised cells
dat_chosen <-
  eig10 |>
  dplyr::filter(grid_cell %in% selected_cells$grid_id) |>
  dplyr::mutate(variable = "selected",
                random_sample = 99) |>
  dplyr::select(grid_id, eig_value, variable)

# get these random samples
dat_compare <-
  lapply(1:length(random_sample), function(x) {
    dplyr::filter(dat_all, grid_id %in% random_sample[[x]]) |>
      mutate(random_sample = x)
  })

# bind together and compare
dat_compare <- 
  dplyr::bind_rows(dplyr::bind_rows(dat_compare), 
                   dat_chosen)

# plot the data
ggplot(data = dat_compare |> dplyr::mutate(random_sample = as.character(random_sample)),
       mapping = aes(x = variable, y = eig_value, colour = random_sample)) +
  geom_point(position = position_dodge(width = 0.2)) +
  xlab(NULL) +
  ylab("Expected Information Gain") +
  theme_meta() +
  theme(legend.position = "none")
```
